{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "751fc4ac",
   "metadata": {},
   "source": [
    "# 02 – Data Cleaning & Feature Engineering\n",
    "\n",
    "**Notebook Name:** `02_Data_Cleaning_Feature_Engineering.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9769276b",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Load raw data from `inputs/datasets/raw/HousePricesRecords.csv`.\n",
    "- Handle nulls and duplicates.\n",
    "- Parse dates into separate Year and Month features.\n",
    "- Map and one-hot encode key categorical fields.\n",
    "- Save the cleaned, feature-engineered dataset for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cf7585",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "- `inputs/datasets/raw/HousePricesRecords.csv` (raw Price-Paid records)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69de4e65",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "- `outputs/datasets/collection/HousePricesRecords_clean.csv` (cleaned and feature-engineered)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c868faa0",
   "metadata": {},
   "source": [
    "## Additional Comments\n",
    "- Follows CRISP-DM Phase 3: Data Preparation.\n",
    "- This cleaned CSV is the source of truth for subsequent analysis notebooks and the Streamlit app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1585c8",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "Loading packages for data manipulation and file operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8eca3ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cd227a",
   "metadata": {},
   "source": [
    "##  Load Raw Data Sample\n",
    "Read a subset of rows in chunks to manage memory.\n",
    "\n",
    "We read in chunks of 200,000 rows and keep the newest 1,000 entries overall for initial inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27444f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../inputs/datasets/raw/price_paid_records.csv\"\n",
    "chunksize = 200_000    \n",
    "topn      = 1_000      \n",
    "latest = None\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    input_path,\n",
    "    usecols=[\n",
    "        \"Price\",\"Date of Transfer\",\"Property Type\",\"Old/New\",\n",
    "        \"Duration\",\"Town/City\",\"County\",\"PPDCategory Type\"\n",
    "    ],\n",
    "    parse_dates=[\"Date of Transfer\"],\n",
    "    chunksize=chunksize,\n",
    "    low_memory=False\n",
    "):\n",
    "    # for this chunk, keep its top `topn` newest rows\n",
    "    chunk_top = chunk.nlargest(topn, \"Date of Transfer\")\n",
    "\n",
    "    if latest is None:\n",
    "        latest = chunk_top\n",
    "    else:\n",
    "        # merge with previous bests, then re-take the top `topn`\n",
    "        latest = pd.concat([latest, chunk_top]).nlargest(topn, \"Date of Transfer\")\n",
    "    \n",
    "\n",
    "df_chunk = latest.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246148f9",
   "metadata": {},
   "source": [
    "### Preview Sample\n",
    "Ensure data loaded correctly; limited to 1,000 rows due to memory constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c20190e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Date of Transfer</th>\n",
       "      <th>Property Type</th>\n",
       "      <th>Old/New</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Town/City</th>\n",
       "      <th>County</th>\n",
       "      <th>PPDCategory Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>277000</td>\n",
       "      <td>2017-06-29</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>WICKFORD</td>\n",
       "      <td>ESSEX</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30000</td>\n",
       "      <td>2017-06-29</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>HULL</td>\n",
       "      <td>CITY OF KINGSTON UPON HULL</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>551000</td>\n",
       "      <td>2017-06-29</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>CHISLEHURST</td>\n",
       "      <td>GREATER LONDON</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>240000</td>\n",
       "      <td>2017-06-29</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>BEDFORD</td>\n",
       "      <td>BEDFORD</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>527500</td>\n",
       "      <td>2017-06-29</td>\n",
       "      <td>D</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>HEMEL HEMPSTEAD</td>\n",
       "      <td>HERTFORDSHIRE</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Price Date of Transfer Property Type Old/New Duration        Town/City  \\\n",
       "0  277000       2017-06-29             S       N        F         WICKFORD   \n",
       "1   30000       2017-06-29             F       N        L             HULL   \n",
       "2  551000       2017-06-29             T       N        F      CHISLEHURST   \n",
       "3  240000       2017-06-29             S       N        F          BEDFORD   \n",
       "4  527500       2017-06-29             D       N        F  HEMEL HEMPSTEAD   \n",
       "\n",
       "                       County PPDCategory Type  \n",
       "0                       ESSEX                A  \n",
       "1  CITY OF KINGSTON UPON HULL                A  \n",
       "2              GREATER LONDON                A  \n",
       "3                     BEDFORD                B  \n",
       "4               HERTFORDSHIRE                B  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0a021a",
   "metadata": {},
   "source": [
    "## Inspect Data Types & Distributions\n",
    "Verify dtypes and explore categorical value counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f57c760a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Data Types:\n",
      "Price                        int64\n",
      "Date of Transfer    datetime64[ns]\n",
      "Property Type               object\n",
      "Old/New                     object\n",
      "Duration                    object\n",
      "Town/City                   object\n",
      "County                      object\n",
      "PPDCategory Type            object\n",
      "dtype: object\n",
      "\n",
      "Value counts for Property Type:\n",
      "Property Type\n",
      "S    286\n",
      "T    282\n",
      "D    239\n",
      "F    140\n",
      "O     53\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for Old/New:\n",
      "Old/New\n",
      "N    992\n",
      "Y      8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for Duration:\n",
      "Duration\n",
      "F    828\n",
      "L    172\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Value counts for County:\n",
      "County\n",
      "GREATER LONDON      79\n",
      "KENT                48\n",
      "HAMPSHIRE           39\n",
      "DEVON               39\n",
      "ESSEX               29\n",
      "                    ..\n",
      "BLACKPOOL            1\n",
      "WARRINGTON           1\n",
      "MIDDLESBROUGH        1\n",
      "POWYS                1\n",
      "ISLE OF ANGLESEY     1\n",
      "Name: count, Length: 103, dtype: int64\n",
      "\n",
      "Value counts for Town/City:\n",
      "Town/City\n",
      "LONDON             51\n",
      "BRISTOL            23\n",
      "NOTTINGHAM         15\n",
      "POOLE              12\n",
      "PLYMOUTH           11\n",
      "                   ..\n",
      "KNEBWORTH           1\n",
      "STAMFORD            1\n",
      "BISHOP AUCKLAND     1\n",
      "PENZANCE            1\n",
      "CAERNARFON          1\n",
      "Name: count, Length: 437, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Column Data Types:\")\n",
    "print(df_chunk.dtypes)\n",
    "\n",
    "categorical_cols = [\"Property Type\", \"Old/New\", \"Duration\", \"County\", \"Town/City\"]\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\nValue counts for {col}:\")\n",
    "    print(df_chunk[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe3dab1",
   "metadata": {},
   "source": [
    "## Clean Missing and Irrelevant Data\n",
    "Remove rows with missing critical fields and drop unused columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4dfd3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "Price               0\n",
      "Date of Transfer    0\n",
      "Property Type       0\n",
      "Old/New             0\n",
      "Duration            0\n",
      "Town/City           0\n",
      "County              0\n",
      "PPDCategory Type    0\n",
      "dtype: int64\n",
      "Shape after dropping missing price/date: (1000, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values in each column:\")\n",
    "print(df_chunk.isnull().sum())\n",
    "\n",
    "df_chunk.dropna(subset=[\"Price\", \"Date of Transfer\"], inplace=True)\n",
    "print(f\"Shape after dropping missing price/date: {df_chunk.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ea2c0",
   "metadata": {},
   "source": [
    "## Date Feature Engineering\n",
    "Convert date to datetime, extract year and month, and filter last 3 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2c9b19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk.drop(columns=[\n",
    "    \"Transaction unique identifier\",\n",
    "    \"District\",\n",
    "    \"Record Status - monthly file only\"\n",
    "], inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "df_chunk[\"Date of Transfer\"] = pd.to_datetime(df_chunk[\"Date of Transfer\"], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5041651",
   "metadata": {},
   "source": [
    "## Encode Categorical Features\n",
    "Map binary and one-hot encode needful fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da9f8e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 0 rows; remaining 1000 rows from 2014-06-29 → 2017-06-29\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_chunk[\"Year\"] = df_chunk[\"Date of Transfer\"].dt.year\n",
    "df_chunk[\"Month\"] = df_chunk[\"Date of Transfer\"].dt.month\n",
    "\n",
    "max_date    = df_chunk[\"Date of Transfer\"].max()\n",
    "cutoff_date = max_date - pd.DateOffset(years=3)\n",
    "\n",
    "# 2) filter to only those transfers within the last 3 years\n",
    "before_rows = df_chunk.shape[0]\n",
    "df_chunk    = df_chunk[df_chunk[\"Date of Transfer\"] >= cutoff_date]\n",
    "after_rows  = df_chunk.shape[0]\n",
    "\n",
    "df_chunk[\"Old/New\"] = df_chunk[\"Old/New\"].map({'N': 0, 'Y': 1})\n",
    "df_chunk[\"Duration\"] = df_chunk[\"Duration\"].map({'F': 1, 'L': 0})\n",
    "df_chunk = pd.get_dummies(df_chunk, columns=[\"Property Type\"], prefix=\"Property\")\n",
    "\n",
    "print(\n",
    "    f\"Filtered out {before_rows - after_rows} rows; \"\n",
    "    f\"remaining {after_rows} rows from {cutoff_date.date()} → {max_date.date()}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b4e7a4",
   "metadata": {},
   "source": [
    "## Remove Duplicates & Add Aggregates\n",
    "Ensure uniqueness and compute region/county-level features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e008fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_chunk.drop_duplicates(inplace=True)\n",
    "df_chunk.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd84da",
   "metadata": {},
   "source": [
    "## Price Transformation & Outlier Removal\n",
    "Log-transform target and remove extreme outliers calculate the value at the 99.5th percentile of LogPrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "581068b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 5 outliers; new shape: (989, 15)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "df_chunk['LogPrice'] = np.log1p(df_chunk['Price'])\n",
    "cutoff = df_chunk['LogPrice'].quantile(0.995)\n",
    "before = df_chunk.shape[0]\n",
    "\n",
    "df_chunk = df_chunk[df_chunk['LogPrice'] <= cutoff]\n",
    "after = df_chunk.shape[0]\n",
    "print(f\"Dropped {before-after} outliers; new shape: {df_chunk.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d7ecb",
   "metadata": {},
   "source": [
    "## Save Cleaned Data\n",
    "Write cleaned DataFrame to CSV for downstream notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ae69cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned data to ../outputs/datasets/collection/HousePricesRecords_clean.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_path = '../outputs/datasets/collection/HousePricesRecords_clean.csv'\n",
    "import os\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "\n",
    "df_chunk.to_csv(output_path, index=False)\n",
    "print(f\"Saved cleaned data to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
