{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "751fc4ac",
   "metadata": {},
   "source": [
    "# 02 – Data Cleaning & Feature Engineering\n",
    "\n",
    "**Notebook Name:** `02_Data_Cleaning_Feature_Engineering.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9769276b",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Load raw data from `inputs/datasets/raw/HousePricesRecords.csv`.\n",
    "- Handle nulls and duplicates.\n",
    "- Parse dates into separate Year and Month features.\n",
    "- Map and one-hot encode key categorical fields.\n",
    "- Save the cleaned, feature-engineered dataset for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cf7585",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "- `inputs/datasets/raw/HousePricesRecords.csv` (raw Price-Paid records)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69de4e65",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "- `outputs/datasets/collection/HousePricesRecords_clean.csv` (cleaned and feature-engineered)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c868faa0",
   "metadata": {},
   "source": [
    "## Additional Comments\n",
    "- Follows CRISP-DM Phase 3: Data Preparation.\n",
    "- This cleaned CSV is the source of truth for subsequent analysis notebooks and the Streamlit app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1585c8",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "Loading packages for data manipulation and file operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eca3ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cd227a",
   "metadata": {},
   "source": [
    "##  Load Raw Data Sample\n",
    "Read a subset of rows in chunks to manage memory.\n",
    "\n",
    "We read in chunks of 200,000 rows and keep the newest 1,000 entries overall for initial inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27444f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../inputs/datasets/raw/price_paid_records.csv\"\n",
    "chunksize = 200_000    \n",
    "topn      = 1_000      \n",
    "latest = None\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    input_path,\n",
    "    usecols=[\n",
    "        \"Price\",\"Date of Transfer\",\"Property Type\",\"Old/New\",\n",
    "        \"Duration\",\"Town/City\",\"County\",\"PPDCategory Type\"\n",
    "    ],\n",
    "    parse_dates=[\"Date of Transfer\"],\n",
    "    chunksize=chunksize,\n",
    "    low_memory=False\n",
    "):\n",
    "    # for this chunk, keep its top `topn` newest rows\n",
    "    chunk_top = chunk.nlargest(topn, \"Date of Transfer\")\n",
    "\n",
    "    if latest is None:\n",
    "        latest = chunk_top\n",
    "    else:\n",
    "        # merge with previous bests, then re-take the top `topn`\n",
    "        latest = pd.concat([latest, chunk_top]).nlargest(topn, \"Date of Transfer\")\n",
    "    \n",
    "\n",
    "df_chunk = latest.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246148f9",
   "metadata": {},
   "source": [
    "### Preview Sample\n",
    "Ensure data loaded correctly; limited to 1,000 rows due to memory constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c20190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0a021a",
   "metadata": {},
   "source": [
    "## Inspect Data Types & Distributions\n",
    "Verify dtypes and explore categorical value counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57c760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Column Data Types:\")\n",
    "print(df_chunk.dtypes)\n",
    "\n",
    "categorical_cols = [\"Property Type\", \"Old/New\", \"Duration\", \"County\", \"Town/City\"]\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\nValue counts for {col}:\")\n",
    "    print(df_chunk[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe3dab1",
   "metadata": {},
   "source": [
    "## Clean Missing and Irrelevant Data\n",
    "Remove rows with missing critical fields and drop unused columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dfd3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in each column:\")\n",
    "print(df_chunk.isnull().sum())\n",
    "\n",
    "df_chunk.dropna(subset=[\"Price\", \"Date of Transfer\"], inplace=True)\n",
    "print(f\"Shape after dropping missing price/date: {df_chunk.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ea2c0",
   "metadata": {},
   "source": [
    "## Date Feature Engineering\n",
    "Convert date to datetime, extract year and month, and filter last 3 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c9b19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk.drop(columns=[\n",
    "    \"Transaction unique identifier\",\n",
    "    \"District\",\n",
    "    \"Record Status - monthly file only\"\n",
    "], inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "df_chunk[\"Date of Transfer\"] = pd.to_datetime(df_chunk[\"Date of Transfer\"], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5041651",
   "metadata": {},
   "source": [
    "## Encode Categorical Features\n",
    "Map binary and one-hot encode needful fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9f8e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_chunk[\"Year\"] = df_chunk[\"Date of Transfer\"].dt.year\n",
    "df_chunk[\"Month\"] = df_chunk[\"Date of Transfer\"].dt.month\n",
    "\n",
    "max_date    = df_chunk[\"Date of Transfer\"].max()\n",
    "cutoff_date = max_date - pd.DateOffset(years=3)\n",
    "\n",
    "# 2) filter to only those transfers within the last 3 years\n",
    "before_rows = df_chunk.shape[0]\n",
    "df_chunk    = df_chunk[df_chunk[\"Date of Transfer\"] >= cutoff_date]\n",
    "after_rows  = df_chunk.shape[0]\n",
    "\n",
    "df_chunk[\"Old/New\"] = df_chunk[\"Old/New\"].map({'N': 0, 'Y': 1})\n",
    "df_chunk[\"Duration\"] = df_chunk[\"Duration\"].map({'F': 1, 'L': 0})\n",
    "df_chunk = pd.get_dummies(df_chunk, columns=[\"Property Type\"], prefix=\"Property\")\n",
    "\n",
    "print(\n",
    "    f\"Filtered out {before_rows - after_rows} rows; \"\n",
    "    f\"remaining {after_rows} rows from {cutoff_date.date()} → {max_date.date()}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b4e7a4",
   "metadata": {},
   "source": [
    "## Remove Duplicates & Add Aggregates\n",
    "Ensure uniqueness and compute region/county-level features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e008fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_chunk.drop_duplicates(inplace=True)\n",
    "df_chunk.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd84da",
   "metadata": {},
   "source": [
    "## Price Transformation & Outlier Removal\n",
    "Log-transform target and remove extreme outliers calculate the value at the 99.5th percentile of LogPrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581068b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "df_chunk['LogPrice'] = np.log1p(df_chunk['Price'])\n",
    "cutoff = df_chunk['LogPrice'].quantile(0.995)\n",
    "before = df_chunk.shape[0]\n",
    "\n",
    "df_chunk = df_chunk[df_chunk['LogPrice'] <= cutoff]\n",
    "after = df_chunk.shape[0]\n",
    "print(f\"Dropped {before-after} outliers; new shape: {df_chunk.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d7ecb",
   "metadata": {},
   "source": [
    "## Save Cleaned Data\n",
    "Write cleaned DataFrame to CSV for downstream notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae69cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_path = 'outputs/datasets/collection/HousePricesRecords_clean.csv'\n",
    "import os\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "\n",
    "df_chunk.to_csv(output_path, index=False)\n",
    "print(f\"Saved cleaned data to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
